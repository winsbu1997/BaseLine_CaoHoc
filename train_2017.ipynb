{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(458968, 84)\n",
      "(170366, 84)\n",
      "168186\n",
      "2180\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from utils.data_preprocessing import load_data, preprocess_data\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "save_data = \"save_data_2017/\"\n",
    "def load_data(file_path):\n",
    "    return pd.read_csv(file_path, encoding='cp1252', engine='python')\n",
    "cicids2017_path = 'data/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv'\n",
    "\n",
    "if(not os.path.isfile(save_data + \"web_attacks.csv\")):\n",
    "    data_2017 = load_data(cicids2017_path)\n",
    "    preprocess_data(data_2017, save_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(save_data + 'web_attacks.csv')\n",
    "df['Label'] = df['Label'].apply(lambda x: 0 if x == 'BENIGN' else 1)\n",
    "\n",
    "\n",
    "normal_df = df[df['Label'] == 0]\n",
    "attack_df = df[df['Label'] != 0]\n",
    "\n",
    "num_attack = len(attack_df)\n",
    "\n",
    "num_normal = 2 * num_attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_df = normal_df.sample(n=num_normal, random_state=42)\n",
    "\n",
    "# Combine and shuffle the dataset\n",
    "balanced_df = pd.concat([normal_df, attack_df]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split the balanced dataset into training (60%), validation (20%), and testing sets (20%)\n",
    "train_df, temp_df = train_test_split(balanced_df, test_size=0.4, random_state=42, stratify=balanced_df['Label'])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['Label'])\n",
    "\n",
    "# Within the training set, ensure normal instances are twice the number of attack instances\n",
    "train_normal_df = train_df[train_df['Label'] == 0]\n",
    "train_attack_df = train_df[train_df['Label'] == 1]\n",
    "num_train_attack = len(train_attack_df)\n",
    "num_train_normal = 2 * num_train_attack\n",
    "train_normal_df = train_normal_df.sample(n=num_train_normal, random_state=42)\n",
    "train_df = pd.concat([train_normal_df, train_attack_df]).sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set class distribution: {0: 2616, 1: 1308}\n",
      "Validation set class distribution: {0: 872, 1: 436}\n",
      "Testing set class distribution: {0: 872, 1: 436}\n"
     ]
    }
   ],
   "source": [
    "# Print the count of each class in the training, validation, and testing sets\n",
    "for name, df in zip([\"Training\", \"Validation\", \"Testing\"], [train_df, val_df, test_df]):\n",
    "    unique, counts = np.unique(df['Label'], return_counts=True)\n",
    "    print(f\"{name} set class distribution:\", dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the train, validation, and test sets to CSV files\n",
    "train_df.to_csv(save_data +  'train_set.csv', index=False)\n",
    "val_df.to_csv(save_data +  'val_set.csv', index=False)\n",
    "test_df.to_csv(save_data + 'test_set.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns (excluding the label column)\n",
    "feature_cols = [col for col in df.columns if col != 'Label']\n",
    "\n",
    "# Extract features and labels for training, validation, and testing sets\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df['Label']\n",
    "X_val = val_df[feature_cols]\n",
    "y_val = val_df['Label']\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classifiers\n",
    "from models.decision_tree import train_decision_tree\n",
    "from models.random_forest import train_random_forest\n",
    "from models.lda import train_lda\n",
    "from models.naive_bayes import train_naive_bayes\n",
    "from models.logistic_regression import train_logistic_regression\n",
    "from models.knn import train_knn\n",
    "from models.svm import train_svm\n",
    "from models.extra_trees import train_extra_trees\n",
    "from models.bagging import train_bagging\n",
    "from models.mlp import train_mlp\n",
    "\n",
    "classifiers = {\n",
    "    'Decision Tree': train_decision_tree,\n",
    "    'Random Forest': train_random_forest,\n",
    "    'Linear Discriminant Analysis': train_lda,\n",
    "    'Naive Bayes': train_naive_bayes,\n",
    "    'Logistic Regression': train_logistic_regression,\n",
    "    'K-Nearest Neighbors': train_knn,\n",
    "    'Support Vector Machine': train_svm,\n",
    "    'Extra Trees Classifier': train_extra_trees,\n",
    "    'Bagging Classifier': train_bagging,\n",
    "    'Multi-layer Perceptron': train_mlp\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Validation - Accuracy: 0.9770642201834863 Precision: 0.9613636363636363 Recall: 0.9701834862385321 F1 Score: 0.9657534246575342\n",
      "Random Forest Validation - Accuracy: 0.9770642201834863 Precision: 0.9634703196347032 Recall: 0.9678899082568807 F1 Score: 0.965675057208238\n",
      "LDA Validation - Accuracy: 0.9503058103975535 Precision: 0.9406175771971497 Recall: 0.908256880733945 F1 Score: 0.9241540256709452\n",
      "Naive Bayes Validation - Accuracy: 0.8165137614678899 Precision: 0.6449704142011834 Recall: 1.0 F1 Score: 0.7841726618705036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\a\\anaconda3\\envs\\AI\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Validation - Accuracy: 0.9686544342507645 Precision: 0.9876543209876543 Recall: 0.9174311926605505 F1 Score: 0.9512485136741974\n",
      "KNN Validation - Accuracy: 0.9640672782874617 Precision: 0.9237472766884531 Recall: 0.9724770642201835 F1 Score: 0.9474860335195531\n",
      "SVM Validation - Accuracy: 0.941131498470948 Precision: 0.9126436781609195 Recall: 0.9105504587155964 F1 Score: 0.9115958668197475\n",
      "Extra Trees Validation - Accuracy: 0.9793577981651376 Precision: 0.9658314350797267 Recall: 0.9724770642201835 F1 Score: 0.9691428571428572\n",
      "MLP Validation - Accuracy: 0.9625382262996942 Precision: 0.9215686274509803 Recall: 0.9701834862385321 F1 Score: 0.9452513966480447\n"
     ]
    }
   ],
   "source": [
    "def model_10Classifier():\n",
    "    results = []\n",
    "    for name, train_function in classifiers.items():\n",
    "        model, val_acc, val_pre, val_rec, val_f1, test_acc, test_pre, test_rec, test_f1 = train_function(X_train, y_train, X_val, y_val, X_test, y_test, save_data)\n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Validation Accuracy': val_acc,\n",
    "            'Validation Precision': val_pre,\n",
    "            'Validation Recall': val_rec,\n",
    "            'Validation F1 Score': val_f1,\n",
    "            \"Test Accuracy\": test_acc,\n",
    "            \"Test Precision\": test_pre,\n",
    "            \"Test Recall\": test_rec,\n",
    "            \"Test F1 Score\": test_f1,\n",
    "        })\n",
    "\n",
    "    # Save results to CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(save_data + '10Classsifer_results_2017.csv', index=False)\n",
    "    \n",
    "model_10Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_pca shape: (3924, 20)\n",
      "X_val_pca shape: (1308, 20)\n",
      "X_test_pca shape: (1308, 20)\n",
      "Decision Tree Validation - Accuracy: 0.9755351681957186 Precision: 0.9654377880184332 Recall: 0.9610091743119266 F1 Score: 0.9632183908045977\n",
      "Random Forest Validation - Accuracy: 0.97782874617737 Precision: 0.967816091954023 Recall: 0.9655963302752294 F1 Score: 0.9667049368541906\n",
      "LDA Validation - Accuracy: 0.9281345565749235 Precision: 0.9476439790575916 Recall: 0.8302752293577982 F1 Score: 0.8850855745721271\n",
      "Naive Bayes Validation - Accuracy: 0.5680428134556575 Precision: 0.42981501632208924 Recall: 0.9059633027522935 F1 Score: 0.5830258302583026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\a\\anaconda3\\envs\\AI\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Validation - Accuracy: 0.9388379204892966 Precision: 0.908256880733945 Recall: 0.908256880733945 F1 Score: 0.908256880733945\n",
      "KNN Validation - Accuracy: 0.9602446483180428 Precision: 0.9247787610619469 Recall: 0.9587155963302753 F1 Score: 0.9414414414414415\n",
      "SVM Validation - Accuracy: 0.9418960244648318 Precision: 0.9147465437788018 Recall: 0.9105504587155964 F1 Score: 0.9126436781609195\n",
      "Extra Trees Validation - Accuracy: 0.9785932721712538 Precision: 0.9700460829493087 Recall: 0.9655963302752294 F1 Score: 0.967816091954023\n",
      "MLP Validation - Accuracy: 0.959480122324159 Precision: 0.9592326139088729 Recall: 0.9174311926605505 F1 Score: 0.9378663540445487\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assume the classifiers dictionary and train functions are already defined\n",
    "# Define a function to apply PCA and train classifiers\n",
    "\n",
    "def train_with_pca(X_train, y_train, X_val, y_val, X_test, y_test, n_components=20):\n",
    "    X_combined = np.concatenate((X_train, X_val, X_test), axis=0)\n",
    "    y_combined = np.concatenate((y_train, y_val, y_test), axis=0)\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_combined_pca = pca.fit_transform(X_combined)\n",
    "\n",
    "    # Bước 3: Tách lại dữ liệu thành các tập train, validation và test\n",
    "    X_train_pca = X_combined_pca[:len(X_train)]\n",
    "    X_val_pca = X_combined_pca[len(X_train):len(X_train)+len(X_val)]\n",
    "    X_test_pca = X_combined_pca[len(X_train)+len(X_val):]\n",
    "\n",
    "    # Kiểm tra kích thước của các tập dữ liệu sau khi PCA\n",
    "    print(\"X_train_pca shape:\", X_train_pca.shape)\n",
    "    print(\"X_val_pca shape:\", X_val_pca.shape)\n",
    "    print(\"X_test_pca shape:\", X_test_pca.shape)\n",
    "    \n",
    "    results = []\n",
    "    for name, train_function in classifiers.items():\n",
    "        model, val_acc, val_pre, val_rec, val_f1, test_acc, test_pre, test_rec, test_f1 = train_function(X_train_pca, y_train, X_val_pca, y_val, X_test_pca, y_test, save_data)\n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Validation Accuracy': val_acc,\n",
    "            'Validation Precision': val_pre,\n",
    "            'Validation Recall': val_rec,\n",
    "            'Validation F1 Score': val_f1,\n",
    "            'Test Accuracy': test_acc,\n",
    "            'Test Precision': test_pre,\n",
    "            'Test Recall': test_rec,\n",
    "            'Test F1 Score': test_f1\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Call the function and save results\n",
    "pca_results = train_with_pca(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "pca_results_df = pd.DataFrame(pca_results)\n",
    "pca_results_df.to_csv(save_data + 'pca_model_results_2017.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - loss: 60702412767232.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 2/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 62191139028992.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 3/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 70830839037952.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 4/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 66795385913344.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 5/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 58237458055168.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 6/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 59354321518592.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 7/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 61542598967296.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 8/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 61115321024512.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 9/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 63248749559808.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 10/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 66400383139840.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 11/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 65517763166208.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 12/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 62415760785408.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 13/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 65367267344384.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 14/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 64871366393856.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 15/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 61246208475136.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 16/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 63939467542528.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 17/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 59028084359168.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 18/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 57631376932864.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 19/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 67060344291328.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 20/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 58178284814336.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 21/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 59355860828160.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 22/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 62283065589760.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 23/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 71434005118976.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 24/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 62074256359424.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 25/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 62597646778368.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 26/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 66245374246912.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 27/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 58690493218816.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 28/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 55945673572352.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 29/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 64177410408448.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 30/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 58610415566848.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 31/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 63780792827904.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 32/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 55999381635072.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 33/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 66402044084224.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 34/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 67157194964992.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 35/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 69013728133120.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 36/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 61686635560960.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 37/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 62447843016704.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 38/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 67342897774592.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 39/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 57268942929920.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 40/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 56706147024896.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 41/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 58403254697984.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 42/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 69143277600768.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 43/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 62794883923968.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 44/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 61898066231296.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 45/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 65559731372032.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 46/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 59210733715456.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 47/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 60163184656384.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 48/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 61166172766208.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 49/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 60477782622208.0000 - val_loss: 46646901604352.0000\n",
      "Epoch 50/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 59306850385920.0000 - val_loss: 46646901604352.0000\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 612us/step\n",
      "Decision Tree Validation - Accuracy: 0.9625382262996942 Precision: 0.9290465631929047 Recall: 0.9610091743119266 F1 Score: 0.9447576099210823\n",
      "Random Forest Validation - Accuracy: 0.9648318042813455 Precision: 0.9295154185022027 Recall: 0.9678899082568807 F1 Score: 0.9483146067415731\n",
      "LDA Validation - Accuracy: 0.9174311926605505 Precision: 0.9183673469387755 Recall: 0.8256880733944955 F1 Score: 0.8695652173913043\n",
      "Naive Bayes Validation - Accuracy: 0.44954128440366975 Precision: 0.37207207207207205 Recall: 0.9472477064220184 F1 Score: 0.5342820181112549\n",
      "Logistic Regression Validation - Accuracy: 0.9288990825688074 Precision: 0.8960739030023095 Recall: 0.8899082568807339 F1 Score: 0.8929804372842347\n",
      "KNN Validation - Accuracy: 0.9571865443425076 Precision: 0.9094827586206896 Recall: 0.9678899082568807 F1 Score: 0.9377777777777778\n",
      "SVM Validation - Accuracy: 0.9311926605504587 Precision: 0.8760869565217392 Recall: 0.9243119266055045 F1 Score: 0.8995535714285714\n",
      "Extra Trees Validation - Accuracy: 0.9694189602446484 Precision: 0.94 Recall: 0.9701834862385321 F1 Score: 0.9548532731376975\n",
      "MLP Validation - Accuracy: 0.9373088685015291 Precision: 0.9317073170731708 Recall: 0.8761467889908257 F1 Score: 0.9030732860520094\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define a function to build and train a deep autoencoder\n",
    "def build_autoencoder(input_dim):\n",
    "    # 3 lớp ẩn\n",
    "    encoding_dim1 = 50\n",
    "    encoding_dim2 = 30\n",
    "    # lớp trung gian\n",
    "    encoding_dim3 = int(np.sqrt(input_dim)) + 1\n",
    "    \n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    \n",
    "    # Lớp mã hóa\n",
    "    encoded = Dense(encoding_dim1, activation='relu')(input_layer)\n",
    "    encoded = Dense(encoding_dim2, activation='relu')(encoded)\n",
    "    encoded = Dense(encoding_dim3, activation='relu')(encoded)\n",
    "    \n",
    "    # Lớp giải mã\n",
    "    decoded = Dense(encoding_dim2, activation='relu')(encoded)\n",
    "    decoded = Dense(encoding_dim1, activation='relu')(decoded)\n",
    "    decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "    \n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    encoder = Model(input_layer, encoded)\n",
    "    \n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    return autoencoder, encoder\n",
    "\n",
    "# Define a function to apply deep autoencoder and train classifiers\n",
    "def train_with_autoencoder(X_train, y_train, X_val, y_val, X_test, y_test,  epochs=50, batch_size=256):\n",
    "    input_dim = X_train.shape[1]\n",
    "    autoencoder, encoder = build_autoencoder(input_dim)\n",
    "    \n",
    "    autoencoder.fit(X_train, X_train, epochs=epochs, batch_size=batch_size, shuffle=True, validation_data=(X_val, X_val), verbose=1)\n",
    "    \n",
    "    X_train_enc = encoder.predict(X_train)\n",
    "    X_val_enc = encoder.predict(X_val)\n",
    "    X_test_enc = encoder.predict(X_test)\n",
    "    \n",
    "    results = []\n",
    "    for name, train_function in classifiers.items():\n",
    "        model, val_acc, val_pre, val_rec, val_f1, test_acc, test_pre, test_rec, test_f1 = train_function(X_train_enc, y_train, X_val_enc, y_val, X_test_enc, y_test, save_data)\n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Validation Accuracy': val_acc,\n",
    "            'Validation Precision': val_pre,\n",
    "            'Validation Recall': val_rec,\n",
    "            'Validation F1 Score': val_f1,\n",
    "            'Test Accuracy': test_acc,\n",
    "            'Test Precision': test_pre,\n",
    "            'Test Recall': test_rec,\n",
    "            'Test F1 Score': test_f1\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Call the function and save results\n",
    "autoencoder_results = train_with_autoencoder(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "autoencoder_results_df = pd.DataFrame(autoencoder_results)\n",
    "autoencoder_results_df.to_csv(save_data + 'autoencoder_model_results_2017.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
