{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils.data_preprocessing import load_data, preprocess_data\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "save_data = \"save_data_2017/\"\n",
    "def load_data(file_path):\n",
    "    return pd.read_csv(file_path, encoding='cp1252', engine='python')\n",
    "cicids2017_path = 'data/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv'\n",
    "\n",
    "if(not os.path.isfile(save_data + \"web_attacks.csv\")):\n",
    "    data_2017 = load_data(cicids2017_path)\n",
    "    preprocess_data(data_2017, save_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(save_data + 'web_attacks.csv')\n",
    "df['Label'] = df['Label'].apply(lambda x: 0 if x == 'BENIGN' else 1)\n",
    "\n",
    "\n",
    "normal_df = df[df['Label'] == 0]\n",
    "attack_df = df[df['Label'] != 0]\n",
    "\n",
    "num_attack = len(attack_df)\n",
    "\n",
    "num_normal = 2 * num_attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_df = normal_df.sample(n=num_normal, random_state=42)\n",
    "\n",
    "# Combine and shuffle the dataset\n",
    "balanced_df = pd.concat([normal_df, attack_df]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split the balanced dataset into training (60%), validation (20%), and testing sets (20%)\n",
    "train_df, temp_df = train_test_split(balanced_df, test_size=0.4, random_state=42, stratify=balanced_df['Label'])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['Label'])\n",
    "\n",
    "# Within the training set, ensure normal instances are twice the number of attack instances\n",
    "train_normal_df = train_df[train_df['Label'] == 0]\n",
    "train_attack_df = train_df[train_df['Label'] == 1]\n",
    "num_train_attack = len(train_attack_df)\n",
    "num_train_normal = 2 * num_train_attack\n",
    "train_normal_df = train_normal_df.sample(n=num_train_normal, random_state=42)\n",
    "train_df = pd.concat([train_normal_df, train_attack_df]).sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set class distribution: {0: 2616, 1: 1308}\n",
      "Validation set class distribution: {0: 872, 1: 436}\n",
      "Testing set class distribution: {0: 872, 1: 436}\n"
     ]
    }
   ],
   "source": [
    "# Print the count of each class in the training, validation, and testing sets\n",
    "for name, df in zip([\"Training\", \"Validation\", \"Testing\"], [train_df, val_df, test_df]):\n",
    "    unique, counts = np.unique(df['Label'], return_counts=True)\n",
    "    print(f\"{name} set class distribution:\", dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the train, validation, and test sets to CSV files\n",
    "train_df.to_csv(save_data +  'train_set.csv', index=False)\n",
    "val_df.to_csv(save_data +  'val_set.csv', index=False)\n",
    "test_df.to_csv(save_data + 'test_set.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns (excluding the label column)\n",
    "feature_cols = [col for col in df.columns if col != 'Label']\n",
    "\n",
    "# Extract features and labels for training, validation, and testing sets\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df['Label']\n",
    "X_val = val_df[feature_cols]\n",
    "y_val = val_df['Label']\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classifiers\n",
    "from models.decision_tree import train_decision_tree\n",
    "from models.random_forest import train_random_forest\n",
    "from models.lda import train_lda\n",
    "from models.naive_bayes import train_naive_bayes\n",
    "from models.logistic_regression import train_logistic_regression\n",
    "from models.knn import train_knn\n",
    "from models.svm import train_svm\n",
    "from models.extra_trees import train_extra_trees\n",
    "from models.bagging import train_bagging\n",
    "from models.mlp import train_mlp\n",
    "\n",
    "classifiers = {\n",
    "    'Decision Tree': train_decision_tree,\n",
    "    'Random Forest': train_random_forest,\n",
    "    'Linear Discriminant Analysis': train_lda,\n",
    "    'Naive Bayes': train_naive_bayes,\n",
    "    'Logistic Regression': train_logistic_regression,\n",
    "    'K-Nearest Neighbors': train_knn,\n",
    "    'Support Vector Machine': train_svm,\n",
    "    'Extra Trees Classifier': train_extra_trees,\n",
    "    'Bagging Classifier': train_bagging,\n",
    "    'Multi-layer Perceptron': train_mlp\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Validation - Accuracy: 0.9938837920489296 Precision: 0.9885844748858448 Recall: 0.9931192660550459 F1 Score: 0.9908466819221968\n",
      "Random Forest Validation - Accuracy: 0.9938837920489296 Precision: 0.9953703703703703 Recall: 0.9862385321100917 F1 Score: 0.9907834101382489\n",
      "LDA Validation - Accuracy: 0.9503058103975535 Precision: 0.9385342789598109 Recall: 0.9105504587155964 F1 Score: 0.9243306169965075\n",
      "Naive Bayes Validation - Accuracy: 0.845565749235474 Precision: 0.6833855799373041 Recall: 1.0 F1 Score: 0.8119180633147114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\a\\anaconda3\\envs\\AI\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Validation - Accuracy: 0.9870030581039755 Precision: 0.9794050343249427 Recall: 0.981651376146789 F1 Score: 0.9805269186712485\n",
      "KNN Validation - Accuracy: 0.9831804281345565 Precision: 0.9620535714285714 Recall: 0.9885321100917431 F1 Score: 0.9751131221719457\n",
      "SVM Validation - Accuracy: 0.941131498470948 Precision: 0.9126436781609195 Recall: 0.9105504587155964 F1 Score: 0.9115958668197475\n",
      "Extra Trees Validation - Accuracy: 0.9969418960244648 Precision: 0.9931506849315068 Recall: 0.9977064220183486 F1 Score: 0.9954233409610984\n",
      "MLP Validation - Accuracy: 0.9655963302752294 Precision: 0.911578947368421 Recall: 0.9931192660550459 F1 Score: 0.9506037321624589\n"
     ]
    }
   ],
   "source": [
    "def model_10Classifier():\n",
    "    results = []\n",
    "    for name, train_function in classifiers.items():\n",
    "        model, val_acc, val_pre, val_rec, val_f1, test_acc, test_pre, test_rec, test_f1 = train_function(X_train, y_train, X_val, y_val, X_test, y_test, save_data)\n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Validation Accuracy': val_acc,\n",
    "            'Validation Precision': val_pre,\n",
    "            'Validation Recall': val_rec,\n",
    "            'Validation F1 Score': val_f1,\n",
    "            \"Test Accuracy\": test_acc,\n",
    "            \"Test Precision\": test_pre,\n",
    "            \"Test Recall\": test_rec,\n",
    "            \"Test F1 Score\": test_f1,\n",
    "        })\n",
    "\n",
    "    # Save results to CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(save_data + '10Classsifer_results_2017.csv', index=False)\n",
    "    \n",
    "model_10Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_pca shape: (3924, 20)\n",
      "X_val_pca shape: (1308, 20)\n",
      "X_test_pca shape: (1308, 20)\n",
      "Decision Tree Validation - Accuracy: 0.985474006116208 Precision: 0.9793103448275862 Recall: 0.9770642201834863 F1 Score: 0.9781859931113662\n",
      "Random Forest Validation - Accuracy: 0.9892966360856269 Precision: 0.9884259259259259 Recall: 0.9793577981651376 F1 Score: 0.9838709677419355\n",
      "LDA Validation - Accuracy: 0.9281345565749235 Precision: 0.9476439790575916 Recall: 0.8302752293577982 F1 Score: 0.8850855745721271\n",
      "Naive Bayes Validation - Accuracy: 0.5680428134556575 Precision: 0.42981501632208924 Recall: 0.9059633027522935 F1 Score: 0.5830258302583026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\a\\anaconda3\\envs\\AI\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Validation - Accuracy: 0.9388379204892966 Precision: 0.908256880733945 Recall: 0.908256880733945 F1 Score: 0.908256880733945\n",
      "KNN Validation - Accuracy: 0.9610091743119266 Precision: 0.9249448123620309 Recall: 0.9610091743119266 F1 Score: 0.9426321709786277\n",
      "SVM Validation - Accuracy: 0.9418960244648318 Precision: 0.9147465437788018 Recall: 0.9105504587155964 F1 Score: 0.9126436781609195\n",
      "Extra Trees Validation - Accuracy: 0.9900611620795107 Precision: 0.9884526558891455 Recall: 0.981651376146789 F1 Score: 0.9850402761795167\n",
      "MLP Validation - Accuracy: 0.9587155963302753 Precision: 0.9569377990430622 Recall: 0.9174311926605505 F1 Score: 0.936768149882904\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assume the classifiers dictionary and train functions are already defined\n",
    "# Define a function to apply PCA and train classifiers\n",
    "\n",
    "def train_with_pca(X_train, y_train, X_val, y_val, X_test, y_test, n_components=20):\n",
    "    X_combined = np.concatenate((X_train, X_val, X_test), axis=0)\n",
    "    y_combined = np.concatenate((y_train, y_val, y_test), axis=0)\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_combined_pca = pca.fit_transform(X_combined)\n",
    "\n",
    "    # Bước 3: Tách lại dữ liệu thành các tập train, validation và test\n",
    "    X_train_pca = X_combined_pca[:len(X_train)]\n",
    "    X_val_pca = X_combined_pca[len(X_train):len(X_train)+len(X_val)]\n",
    "    X_test_pca = X_combined_pca[len(X_train)+len(X_val):]\n",
    "\n",
    "    # Kiểm tra kích thước của các tập dữ liệu sau khi PCA\n",
    "    print(\"X_train_pca shape:\", X_train_pca.shape)\n",
    "    print(\"X_val_pca shape:\", X_val_pca.shape)\n",
    "    print(\"X_test_pca shape:\", X_test_pca.shape)\n",
    "    \n",
    "    results = []\n",
    "    for name, train_function in classifiers.items():\n",
    "        model, val_acc, val_pre, val_rec, val_f1, test_acc, test_pre, test_rec, test_f1 = train_function(X_train_pca, y_train, X_val_pca, y_val, X_test_pca, y_test, save_data)\n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Validation Accuracy': val_acc,\n",
    "            'Validation Precision': val_pre,\n",
    "            'Validation Recall': val_rec,\n",
    "            'Validation F1 Score': val_f1,\n",
    "            'Test Accuracy': test_acc,\n",
    "            'Test Precision': test_pre,\n",
    "            'Test Recall': test_rec,\n",
    "            'Test F1 Score': test_f1\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Call the function and save results\n",
    "pca_results = train_with_pca(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "pca_results_df = pd.DataFrame(pca_results)\n",
    "pca_results_df.to_csv(save_data + 'pca_model_results_2017.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 5841290149888.0\n",
      "Epoch [2/50], Loss: 5798083837952.0\n",
      "Epoch [3/50], Loss: 5910991552512.0\n",
      "Epoch [4/50], Loss: 5722112278528.0\n",
      "Epoch [5/50], Loss: 5803935481856.0\n",
      "Epoch [6/50], Loss: 5975964778496.0\n",
      "Epoch [7/50], Loss: 5692207988736.0\n",
      "Epoch [8/50], Loss: 5802480599040.0\n",
      "Epoch [9/50], Loss: 5680468410368.0\n",
      "Epoch [10/50], Loss: 5802610343936.0\n",
      "Epoch [11/50], Loss: 5742305280000.0\n",
      "Epoch [12/50], Loss: 5887142690816.0\n",
      "Epoch [13/50], Loss: 6496283344896.0\n",
      "Epoch [14/50], Loss: 5795666132992.0\n",
      "Epoch [15/50], Loss: 5840801218560.0\n",
      "Epoch [16/50], Loss: 5706979983360.0\n",
      "Epoch [17/50], Loss: 5907976192000.0\n",
      "Epoch [18/50], Loss: 5834244571136.0\n",
      "Epoch [19/50], Loss: 5789612720128.0\n",
      "Epoch [20/50], Loss: 5680554344448.0\n",
      "Epoch [21/50], Loss: 5931684085760.0\n",
      "Epoch [22/50], Loss: 5873183703040.0\n",
      "Epoch [23/50], Loss: 5803325095936.0\n",
      "Epoch [24/50], Loss: 5821406478336.0\n",
      "Epoch [25/50], Loss: 5753672155136.0\n",
      "Epoch [26/50], Loss: 5882027491328.0\n",
      "Epoch [27/50], Loss: 5720095883264.0\n",
      "Epoch [28/50], Loss: 5653011251200.0\n",
      "Epoch [29/50], Loss: 6101785362432.0\n",
      "Epoch [30/50], Loss: 5805326843904.0\n",
      "Epoch [31/50], Loss: 5737262137344.0\n",
      "Epoch [32/50], Loss: 5857693220864.0\n",
      "Epoch [33/50], Loss: 5727872417792.0\n",
      "Epoch [34/50], Loss: 5985981546496.0\n",
      "Epoch [35/50], Loss: 5878262775808.0\n",
      "Epoch [36/50], Loss: 5685572108288.0\n",
      "Epoch [37/50], Loss: 5863023181824.0\n",
      "Epoch [38/50], Loss: 5700776509440.0\n",
      "Epoch [39/50], Loss: 5751652433920.0\n",
      "Epoch [40/50], Loss: 5877142388736.0\n",
      "Epoch [41/50], Loss: 5761701314560.0\n",
      "Epoch [42/50], Loss: 5919924568064.0\n",
      "Epoch [43/50], Loss: 6201449168896.0\n",
      "Epoch [44/50], Loss: 5777080631296.0\n",
      "Epoch [45/50], Loss: 5778612994048.0\n",
      "Epoch [46/50], Loss: 5779861880832.0\n",
      "Epoch [47/50], Loss: 6022638944256.0\n",
      "Epoch [48/50], Loss: 5830529007616.0\n",
      "Epoch [49/50], Loss: 5953447477248.0\n",
      "Epoch [50/50], Loss: 5905669947392.0\n",
      "Decision Tree Validation - Accuracy: 0.97782874617737 Precision: 0.9512195121951219 Recall: 0.9839449541284404 F1 Score: 0.9673055242390078\n",
      "Random Forest Validation - Accuracy: 0.981651376146789 Precision: 0.9681818181818181 Recall: 0.9770642201834863 F1 Score: 0.9726027397260274\n",
      "LDA Validation - Accuracy: 0.922782874617737 Precision: 0.9373368146214099 Recall: 0.823394495412844 F1 Score: 0.8766788766788767\n",
      "Naive Bayes Validation - Accuracy: 0.42660550458715596 Precision: 0.36155202821869487 Recall: 0.9403669724770642 F1 Score: 0.5222929936305732\n",
      "Logistic Regression Validation - Accuracy: 0.9235474006116208 Precision: 0.8981042654028436 Recall: 0.8692660550458715 F1 Score: 0.8834498834498834\n",
      "KNN Validation - Accuracy: 0.9762996941896025 Precision: 0.9470198675496688 Recall: 0.9839449541284404 F1 Score: 0.9651293588301463\n",
      "SVM Validation - Accuracy: 0.9396024464831805 Precision: 0.9122401847575058 Recall: 0.9059633027522935 F1 Score: 0.9090909090909091\n",
      "Extra Trees Validation - Accuracy: 0.985474006116208 Precision: 0.9706546275395034 Recall: 0.9862385321100917 F1 Score: 0.9783845278725825\n",
      "MLP Validation - Accuracy: 0.8631498470948012 Precision: 0.7429111531190926 Recall: 0.9013761467889908 F1 Score: 0.8145077720207254\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Định nghĩa Autoencoder\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        encoding_dim1 = 50\n",
    "        encoding_dim2 = 30\n",
    "        encoding_dim3 = int(np.sqrt(input_dim)) + 1\n",
    "\n",
    "        # Lớp mã hóa\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, encoding_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim1, encoding_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim2, encoding_dim3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Lớp giải mã\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim3, encoding_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim2, encoding_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim1, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "# Hàm để áp dụng deep autoencoder và train classifiers\n",
    "def train_with_autoencoder(X_train, y_train, X_val, y_val, X_test, y_test, epochs=50, batch_size=256, alpha1=0.1):\n",
    "    input_dim = X_train.shape[1]\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Khởi tạo mô hình và các thành phần cần thiết\n",
    "    autoencoder = Autoencoder(input_dim).to(device)\n",
    "    optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Chuyển đổi dữ liệu DataFrame thành NumPy array và tensor\n",
    "    X_train_np = X_train.to_numpy()\n",
    "    X_val_np = X_val.to_numpy()\n",
    "    X_test_np = X_test.to_numpy()\n",
    "    y_train_np = y_train.to_numpy()\n",
    "    y_val_np = y_val.to_numpy()\n",
    "    y_test_np = y_test.to_numpy()\n",
    "    \n",
    "    # Tạo TensorDataset cho training và validation\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.tensor(X_train_np, dtype=torch.float32),\n",
    "        torch.tensor(X_train_np, dtype=torch.float32)  # Sử dụng dữ liệu đầu vào làm mục tiêu cho autoencoder\n",
    "    )\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.tensor(X_val_np, dtype=torch.float32),\n",
    "        torch.tensor(X_val_np, dtype=torch.float32)  # Sử dụng dữ liệu đầu vào làm mục tiêu cho autoencoder\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        autoencoder.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            encoded, outputs = autoencoder(inputs)\n",
    "            loss = alpha1 * criterion(outputs, targets)  # Chỉ sử dụng MSE loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {train_loss/len(train_loader)}')\n",
    "\n",
    "    # Encode dữ liệu\n",
    "    autoencoder.eval()\n",
    "    with torch.no_grad():\n",
    "        X_train_enc, _ = autoencoder(torch.tensor(X_train_np, dtype=torch.float32).to(device))\n",
    "        X_val_enc, _ = autoencoder(torch.tensor(X_val_np, dtype=torch.float32).to(device))\n",
    "        X_test_enc, _ = autoencoder(torch.tensor(X_test_np, dtype=torch.float32).to(device))\n",
    "    \n",
    "    # Train và đánh giá classifiers\n",
    "    results = []\n",
    "    for name, train_function in classifiers.items():\n",
    "        model, val_acc, val_pre, val_rec, val_f1, test_acc, test_pre, test_rec, test_f1 = train_function(X_train_enc.cpu().numpy(), y_train, X_val_enc.cpu().numpy(), y_val, X_test_enc.cpu().numpy(), y_test, save_data)\n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Validation Accuracy': val_acc,\n",
    "            'Validation Precision': val_pre,\n",
    "            'Validation Recall': val_rec,\n",
    "            'Validation F1 Score': val_f1,\n",
    "            'Test Accuracy': test_acc,\n",
    "            'Test Precision': test_pre,\n",
    "            'Test Recall': test_rec,\n",
    "            'Test F1 Score': test_f1\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Gọi hàm và lưu kết quả\n",
    "autoencoder_results = train_with_autoencoder(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "autoencoder_results_df = pd.DataFrame(autoencoder_results)\n",
    "autoencoder_results_df.to_csv(save_data + 'autoencoder_model_results.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 11.087749660015106\n",
      "Epoch [2/10], Loss: 10.836863934993744\n",
      "Epoch [3/10], Loss: 10.908373057842255\n",
      "Epoch [4/10], Loss: 10.80006217956543\n",
      "Epoch [5/10], Loss: 10.778883874416351\n",
      "Epoch [6/10], Loss: 10.565011441707611\n",
      "Epoch [7/10], Loss: 10.519079506397247\n",
      "Epoch [8/10], Loss: 10.497524201869965\n",
      "Epoch [9/10], Loss: 10.499695062637329\n",
      "Epoch [10/10], Loss: 10.492914974689484\n",
      "Decision Tree Validation - Accuracy: 0.9709480122324159 Precision: 0.9481981981981982 Recall: 0.9655963302752294 F1 Score: 0.9568181818181818\n",
      "Random Forest Validation - Accuracy: 0.9747706422018348 Precision: 0.9569160997732427 Recall: 0.9678899082568807 F1 Score: 0.9623717217787914\n",
      "LDA Validation - Accuracy: 0.9296636085626911 Precision: 0.9387755102040817 Recall: 0.8440366972477065 F1 Score: 0.8888888888888888\n",
      "Naive Bayes Validation - Accuracy: 0.44571865443425074 Precision: 0.3710972346119536 Recall: 0.9541284403669725 F1 Score: 0.5343609505459217\n",
      "Logistic Regression Validation - Accuracy: 0.9288990825688074 Precision: 0.9341772151898734 Recall: 0.8463302752293578 F1 Score: 0.8880866425992779\n",
      "KNN Validation - Accuracy: 0.9694189602446484 Precision: 0.94 Recall: 0.9701834862385321 F1 Score: 0.9548532731376975\n",
      "SVM Validation - Accuracy: 0.9480122324159022 Precision: 0.9380952380952381 Recall: 0.9036697247706422 F1 Score: 0.9205607476635514\n",
      "Extra Trees Validation - Accuracy: 0.9785932721712538 Precision: 0.9594594594594594 Recall: 0.9770642201834863 F1 Score: 0.9681818181818181\n",
      "MLP Validation - Accuracy: 0.9357798165137615 Precision: 0.9 Recall: 0.908256880733945 F1 Score: 0.9041095890410958\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Định nghĩa Autoencoder\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        encoding_dim1 = 50\n",
    "        encoding_dim2 = 30\n",
    "        encoding_dim3 = int(np.sqrt(input_dim)) + 1\n",
    "\n",
    "        # Lớp mã hóa\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, encoding_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim1, encoding_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim2, encoding_dim3),\n",
    "            nn.ReLU()  # Đổi từ sigmoid thành ReLU để giữ giá trị dương\n",
    "        )\n",
    "        # Lớp giải mã\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim3, encoding_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim2, encoding_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim1, input_dim),\n",
    "            nn.Sigmoid()  # Giữ sigmoid cho đầu ra\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded, encoded\n",
    "\n",
    "class BinaryMagnetLoss(nn.Module):\n",
    "    def __init__(self, alpha=7.18, epsilon=1e-6):\n",
    "        super(BinaryMagnetLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, embeddings, labels):\n",
    "        _min_float = 1e-6\n",
    "        embeddings = embeddings.float()\n",
    "        batch_size = embeddings.size(0)\n",
    "\n",
    "        unique_labels = labels.unique()\n",
    "        batch_clusters = {label.item(): (labels == label).nonzero(as_tuple=True)[0] for label in unique_labels}\n",
    "\n",
    "        num_instances = 0.0\n",
    "        stdev = torch.zeros(1).to(embeddings.device)\n",
    "        c_means = torch.stack([torch.mean(embeddings[batch_clusters[label.item()]], dim=0) for label in unique_labels]).to(embeddings.device)\n",
    "\n",
    "        for label in unique_labels:\n",
    "            cluster_indices = batch_clusters[label.item()]\n",
    "            for i in cluster_indices:\n",
    "                stdev += (embeddings[i] - c_means[unique_labels.tolist().index(label)]).norm(p=2).pow(2)\n",
    "                num_instances += 1.0\n",
    "\n",
    "        stdev = stdev / (num_instances - 1.0)\n",
    "        stdev = -2.0 * (stdev + self.epsilon)  # Add epsilon to avoid division by zero\n",
    "\n",
    "        loss = torch.zeros(1).to(embeddings.device)\n",
    "        denom = [torch.zeros(1).to(embeddings.device) for _ in range(batch_size)]\n",
    "\n",
    "        for label in unique_labels:\n",
    "            cluster_indices = batch_clusters[label.item()]\n",
    "            for i in cluster_indices:\n",
    "                for other_label in unique_labels:\n",
    "                    if other_label != label:\n",
    "                        denom[i] += ((embeddings[i] - c_means[unique_labels.tolist().index(other_label)]).norm().pow(2) / stdev).exp()\n",
    "\n",
    "                # Avoid log(0) by clamping values\n",
    "                loss_term = (((embeddings[i] - c_means[unique_labels.tolist().index(label)]).norm().pow(2) / stdev - self.alpha).exp() / (denom[i] + self.epsilon)).log().clamp(max=0.0)\n",
    "                if torch.isnan(loss_term) or torch.isinf(loss_term):\n",
    "                    continue  # Skip this term if it's invalid\n",
    "                loss -= loss_term\n",
    "\n",
    "        loss /= num_instances\n",
    "        return loss\n",
    "\n",
    "\n",
    "# Custom Loss Function combining Cross-Entropy and Binary Magnet Loss\n",
    "def custom_loss(y_true, y_pred, embeddings, alpha1=1.0, alpha2=1.0):\n",
    "    ce_loss = F.cross_entropy(y_pred, y_true.long())  # Tính toán Cross-Entropy Loss\n",
    "    magnet_loss_fn = BinaryMagnetLoss()\n",
    "    magnet_loss_value = magnet_loss_fn(embeddings, y_true)\n",
    "    return alpha1 * ce_loss + alpha2 * magnet_loss_value\n",
    "\n",
    "# Hàm để áp dụng deep autoencoder và train classifiers\n",
    "def train_with_autoencoder(X_train, y_train, X_val, y_val, X_test, y_test, classifiers, save_data, epochs=10, batch_size=256, alpha1=1.0, alpha2=1.0):\n",
    "    input_dim = X_train.shape[1]\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    autoencoder = Autoencoder(input_dim).to(device)\n",
    "    optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "\n",
    "    X_train_np = X_train.to_numpy()\n",
    "    X_val_np = X_val.to_numpy()\n",
    "    y_train_np = y_train.to_numpy()\n",
    "    y_val_np = y_val.to_numpy()\n",
    "\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.tensor(X_train_np, dtype=torch.float32),\n",
    "        torch.tensor(y_train_np, dtype=torch.float32)\n",
    "    )\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.tensor(X_val_np, dtype=torch.float32),\n",
    "        torch.tensor(y_val_np, dtype=torch.float32)\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        autoencoder.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs, encoded = autoencoder(inputs)\n",
    "            loss = custom_loss(labels, outputs, encoded, alpha1=alpha1, alpha2=alpha2)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {train_loss/len(train_loader)}')\n",
    "\n",
    "    autoencoder.eval()\n",
    "    with torch.no_grad():\n",
    "        X_train_enc = autoencoder.encoder(torch.tensor(X_train_np, dtype=torch.float32).to(device)).cpu().numpy()\n",
    "        X_val_enc = autoencoder.encoder(torch.tensor(X_val_np, dtype=torch.float32).to(device)).cpu().numpy()\n",
    "        X_test_enc = autoencoder.encoder(torch.tensor(X_test.to_numpy(), dtype=torch.float32).to(device)).cpu().numpy()\n",
    "\n",
    "    results = []\n",
    "    for name, train_function in classifiers.items():\n",
    "        model, val_acc, val_pre, val_rec, val_f1, test_acc, test_pre, test_rec, test_f1 = train_function(X_train_enc, y_train, X_val_enc, y_val, X_test_enc, y_test, save_data)\n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Validation Accuracy': val_acc,\n",
    "            'Validation Precision': val_pre,\n",
    "            'Validation Recall': val_rec,\n",
    "            'Validation F1 Score': val_f1,\n",
    "            'Test Accuracy': test_acc,\n",
    "            'Test Precision': test_pre,\n",
    "            'Test Recall': test_rec,\n",
    "            'Test F1 Score': test_f1\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Gọi hàm và lưu kết quả\n",
    "autoencoder_results = train_with_autoencoder(X_train, y_train, X_val, y_val, X_test, y_test, classifiers, save_data)\n",
    "autoencoder_results_df = pd.DataFrame(autoencoder_results)\n",
    "autoencoder_results_df.to_csv(save_data + 'autoencoder_Binary_Magnet_Loss_results.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
