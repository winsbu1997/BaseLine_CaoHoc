{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils.data_preprocessing import load_data, preprocess_data\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "save_data = \"save_data_2018/\"\n",
    "\n",
    "\n",
    "cicids2018_path_v1 = 'data/Thursday-22-02-2018_TrafficForML_CICFlowMeter.csv'\n",
    "cicids2018_path_v2 = 'data/Friday-23-02-2018_TrafficForML_CICFlowMeter.csv'\n",
    "merge_data = object\n",
    "if(not os.path.isfile(save_data + \"merge.csv\")):\n",
    "    # Optionally, remove duplicate rows\n",
    "    data_2018_v1 = load_data(cicids2018_path_v1)\n",
    "    data_2018_v2 = load_data(cicids2018_path_v2)\n",
    "\n",
    "    print(\"Headers match. Proceeding with merge.\")\n",
    "    merged_df = pd.concat([data_2018_v1, data_2018_v2], axis=0, ignore_index=True)\n",
    "    merged_df = merged_df.drop_duplicates()\n",
    "    merged_df.to_csv(save_data + \"merge.csv\", index=False)\n",
    "    \n",
    "    merge_data = load_data(save_data + \"merge.csv\")\n",
    "    \n",
    "if(not os.path.isfile(save_data + \"web_attacks.csv\")):\n",
    "    preprocess_data(merge_data, save_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(save_data + 'web_attacks.csv')\n",
    "df['Label'] = df['Label'].apply(lambda x: 0 if x == 'Benign' else 1)\n",
    "\n",
    "\n",
    "normal_df = df[df['Label'] == 0]\n",
    "attack_df = df[df['Label'] != 0]\n",
    "\n",
    "num_attack = len(attack_df)\n",
    "\n",
    "num_normal = 2 * num_attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if num_normal > len(normal_df):\n",
    "    raise ValueError(\"num_normal exceeds the number of rows in normal_df.\")\n",
    "normal_df = normal_df.sample(n=num_normal, random_state=42)\n",
    "\n",
    "# Ensure 'Label' column exists\n",
    "if 'Label' not in normal_df.columns or 'Label' not in attack_df.columns:\n",
    "    raise ValueError(\"Both DataFrames must have a 'Label' column.\")\n",
    "\n",
    "# Combine and shuffle the dataset\n",
    "balanced_df = pd.concat([normal_df, attack_df]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split the balanced dataset\n",
    "if 'Label' not in balanced_df.columns:\n",
    "    raise ValueError(\"The 'Label' column is missing from the balanced DataFrame.\")\n",
    "train_df, temp_df = train_test_split(balanced_df, test_size=0.4, random_state=42, stratify=balanced_df['Label'])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['Label'])\n",
    "\n",
    "# Ensure enough data for balancing\n",
    "train_normal_df = train_df[train_df['Label'] == 0]\n",
    "train_attack_df = train_df[train_df['Label'] == 1]\n",
    "\n",
    "if len(train_attack_df) == 0:\n",
    "    raise ValueError(\"There are no attack instances in the training set.\")\n",
    "\n",
    "num_train_attack = len(train_attack_df)\n",
    "num_train_normal = 2 * num_train_attack\n",
    "\n",
    "# Ensure we don't try to sample more rows than are available\n",
    "if len(train_normal_df) < num_train_normal:\n",
    "    num_train_normal = len(train_normal_df)  # Adjust to available number of rows\n",
    "\n",
    "train_normal_df = train_normal_df.sample(n=num_train_normal, random_state=42)\n",
    "train_df = pd.concat([train_normal_df, train_attack_df]).sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set class distribution: {0: 1113, 1: 557}\n",
      "Validation set class distribution: {0: 371, 1: 186}\n",
      "Testing set class distribution: {0: 372, 1: 185}\n"
     ]
    }
   ],
   "source": [
    "# Print the count of each class in the training, validation, and testing sets\n",
    "for name, df in zip([\"Training\", \"Validation\", \"Testing\"], [train_df, val_df, test_df]):\n",
    "    unique, counts = np.unique(df['Label'], return_counts=True)\n",
    "    print(f\"{name} set class distribution:\", dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the train, validation, and test sets to CSV files\n",
    "train_df.to_csv(save_data +  'train_set.csv', index=False)\n",
    "val_df.to_csv(save_data +  'val_set.csv', index=False)\n",
    "test_df.to_csv(save_data + 'test_set.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns (excluding the label column)\n",
    "feature_cols = [col for col in df.columns if col != 'Label']\n",
    "\n",
    "# Extract features and labels for training, validation, and testing sets\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df['Label']\n",
    "X_val = val_df[feature_cols]\n",
    "y_val = val_df['Label']\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classifiers\n",
    "from models.decision_tree import train_decision_tree\n",
    "from models.random_forest import train_random_forest\n",
    "from models.lda import train_lda\n",
    "from models.naive_bayes import train_naive_bayes\n",
    "from models.logistic_regression import train_logistic_regression\n",
    "from models.knn import train_knn\n",
    "from models.svm import train_svm\n",
    "from models.extra_trees import train_extra_trees\n",
    "from models.bagging import train_bagging\n",
    "from models.mlp import train_mlp\n",
    "\n",
    "classifiers = {\n",
    "    'Decision Tree': train_decision_tree,\n",
    "    'Random Forest': train_random_forest,\n",
    "    'Linear Discriminant Analysis': train_lda,\n",
    "    'Naive Bayes': train_naive_bayes,\n",
    "    'Logistic Regression': train_logistic_regression,\n",
    "    'K-Nearest Neighbors': train_knn,\n",
    "    'Support Vector Machine': train_svm,\n",
    "    'Extra Trees Classifier': train_extra_trees,\n",
    "    'Bagging Classifier': train_bagging,\n",
    "    'Multi-layer Perceptron': train_mlp\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Validation - Accuracy: 0.9892280071813285 Precision: 0.9736842105263158 Recall: 0.9946236559139785 F1 Score: 0.9840425531914894\n",
      "Random Forest Validation - Accuracy: 0.9946140035906643 Precision: 0.9893048128342246 Recall: 0.9946236559139785 F1 Score: 0.9919571045576407\n",
      "LDA Validation - Accuracy: 0.9443447037701975 Precision: 0.8817733990147784 Recall: 0.9623655913978495 F1 Score: 0.9203084832904884\n",
      "Naive Bayes Validation - Accuracy: 0.7091561938958707 Precision: 0.536144578313253 Recall: 0.956989247311828 F1 Score: 0.6872586872586872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\a\\anaconda3\\envs\\AI\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Validation - Accuracy: 0.8186714542190305 Precision: 0.84 Recall: 0.5645161290322581 F1 Score: 0.6752411575562701\n",
      "KNN Validation - Accuracy: 0.9587073608617595 Precision: 0.9095477386934674 Recall: 0.9731182795698925 F1 Score: 0.9402597402597402\n",
      "SVM Validation - Accuracy: 0.7917414721723519 Precision: 0.9861111111111112 Recall: 0.3817204301075269 F1 Score: 0.5503875968992248\n",
      "Extra Trees Validation - Accuracy: 0.9964093357271095 Precision: 0.9893617021276596 Recall: 1.0 F1 Score: 0.9946524064171123\n",
      "MLP Validation - Accuracy: 0.8922800718132855 Precision: 0.8387096774193549 Recall: 0.8387096774193549 F1 Score: 0.8387096774193549\n"
     ]
    }
   ],
   "source": [
    "def model_10Classifier():\n",
    "    results = []\n",
    "    for name, train_function in classifiers.items():\n",
    "        model, val_acc, val_pre, val_rec, val_f1, test_acc, test_pre, test_rec, test_f1 = train_function(X_train, y_train, X_val, y_val, X_test, y_test, save_data)\n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Validation Accuracy': val_acc,\n",
    "            'Validation Precision': val_pre,\n",
    "            'Validation Recall': val_rec,\n",
    "            'Validation F1 Score': val_f1,\n",
    "            \"Test Accuracy\": test_acc,\n",
    "            \"Test Precision\": test_pre,\n",
    "            \"Test Recall\": test_rec,\n",
    "            \"Test F1 Score\": test_f1,\n",
    "        })\n",
    "\n",
    "    # Save results to CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(save_data + '10Classsifer_results_2018.csv', index=False)\n",
    "    \n",
    "model_10Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_pca shape: (1670, 20)\n",
      "X_val_pca shape: (557, 20)\n",
      "X_test_pca shape: (557, 20)\n",
      "Decision Tree Validation - Accuracy: 0.9587073608617595 Precision: 0.9405405405405406 Recall: 0.9354838709677419 F1 Score: 0.9380053908355795\n",
      "Random Forest Validation - Accuracy: 0.9694793536804309 Precision: 0.9567567567567568 Recall: 0.9516129032258065 F1 Score: 0.954177897574124\n",
      "LDA Validation - Accuracy: 0.7809694793536804 Precision: 0.8902439024390244 Recall: 0.3924731182795699 F1 Score: 0.5447761194029851\n",
      "Naive Bayes Validation - Accuracy: 0.39676840215439857 Precision: 0.33766233766233766 Recall: 0.8387096774193549 F1 Score: 0.48148148148148145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\a\\anaconda3\\envs\\AI\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Validation - Accuracy: 0.8186714542190305 Precision: 0.8695652173913043 Recall: 0.5376344086021505 F1 Score: 0.6644518272425249\n",
      "KNN Validation - Accuracy: 0.9317773788150808 Precision: 0.9021739130434783 Recall: 0.8924731182795699 F1 Score: 0.8972972972972973\n",
      "SVM Validation - Accuracy: 0.7917414721723519 Precision: 0.9861111111111112 Recall: 0.3817204301075269 F1 Score: 0.5503875968992248\n",
      "Extra Trees Validation - Accuracy: 0.9640933572710951 Precision: 0.9414893617021277 Recall: 0.9516129032258065 F1 Score: 0.946524064171123\n",
      "MLP Validation - Accuracy: 0.8635547576301615 Precision: 0.9661016949152542 Recall: 0.6129032258064516 F1 Score: 0.75\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assume the classifiers dictionary and train functions are already defined\n",
    "# Define a function to apply PCA and train classifiers\n",
    "\n",
    "def train_with_pca(X_train, y_train, X_val, y_val, X_test, y_test, n_components=20):\n",
    "    X_combined = np.concatenate((X_train, X_val, X_test), axis=0)\n",
    "    y_combined = np.concatenate((y_train, y_val, y_test), axis=0)\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_combined_pca = pca.fit_transform(X_combined)\n",
    "\n",
    "    # Bước 3: Tách lại dữ liệu thành các tập train, validation và test\n",
    "    X_train_pca = X_combined_pca[:len(X_train)]\n",
    "    X_val_pca = X_combined_pca[len(X_train):len(X_train)+len(X_val)]\n",
    "    X_test_pca = X_combined_pca[len(X_train)+len(X_val):]\n",
    "\n",
    "    # Kiểm tra kích thước của các tập dữ liệu sau khi PCA\n",
    "    print(\"X_train_pca shape:\", X_train_pca.shape)\n",
    "    print(\"X_val_pca shape:\", X_val_pca.shape)\n",
    "    print(\"X_test_pca shape:\", X_test_pca.shape)\n",
    "    \n",
    "    results = []\n",
    "    for name, train_function in classifiers.items():\n",
    "        model, val_acc, val_pre, val_rec, val_f1, test_acc, test_pre, test_rec, test_f1 = train_function(X_train_pca, y_train, X_val_pca, y_val, X_test_pca, y_test, save_data)\n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Validation Accuracy': val_acc,\n",
    "            'Validation Precision': val_pre,\n",
    "            'Validation Recall': val_rec,\n",
    "            'Validation F1 Score': val_f1,\n",
    "            'Test Accuracy': test_acc,\n",
    "            'Test Precision': test_pre,\n",
    "            'Test Recall': test_rec,\n",
    "            'Test F1 Score': test_f1\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Call the function and save results\n",
    "pca_results = train_with_pca(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "pca_results_df = pd.DataFrame(pca_results)\n",
    "pca_results_df.to_csv(save_data + 'pca_model_results_2018.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 11936803136658.285\n",
      "Epoch [2/50], Loss: 11967147877522.285\n",
      "Epoch [3/50], Loss: 11992866450870.857\n",
      "Epoch [4/50], Loss: 12077963935744.0\n",
      "Epoch [5/50], Loss: 11983782936576.0\n",
      "Epoch [6/50], Loss: 11876845560978.285\n",
      "Epoch [7/50], Loss: 11975827839853.715\n",
      "Epoch [8/50], Loss: 12035234313947.428\n",
      "Epoch [9/50], Loss: 11904329187328.0\n",
      "Epoch [10/50], Loss: 12057901306441.143\n",
      "Epoch [11/50], Loss: 12062069021549.715\n",
      "Epoch [12/50], Loss: 12201278121106.285\n",
      "Epoch [13/50], Loss: 11879714764507.428\n",
      "Epoch [14/50], Loss: 12027118934893.715\n",
      "Epoch [15/50], Loss: 12091767839597.715\n",
      "Epoch [16/50], Loss: 11714842666130.285\n",
      "Epoch [17/50], Loss: 12069116500845.715\n",
      "Epoch [18/50], Loss: 11949041965933.715\n",
      "Epoch [19/50], Loss: 11927130622244.572\n",
      "Epoch [20/50], Loss: 11870434866907.428\n",
      "Epoch [21/50], Loss: 11959366994212.572\n",
      "Epoch [22/50], Loss: 11958180605366.857\n",
      "Epoch [23/50], Loss: 12306422806235.428\n",
      "Epoch [24/50], Loss: 11871630318445.715\n",
      "Epoch [25/50], Loss: 11894611584146.285\n",
      "Epoch [26/50], Loss: 11971828271396.572\n",
      "Epoch [27/50], Loss: 12079282594962.285\n",
      "Epoch [28/50], Loss: 12217119258331.428\n",
      "Epoch [29/50], Loss: 11961130848841.143\n",
      "Epoch [30/50], Loss: 12296441486189.715\n",
      "Epoch [31/50], Loss: 12022818574921.143\n",
      "Epoch [32/50], Loss: 12055047007670.857\n",
      "Epoch [33/50], Loss: 11945149502025.143\n",
      "Epoch [34/50], Loss: 12052429312585.143\n",
      "Epoch [35/50], Loss: 11715221726354.285\n",
      "Epoch [36/50], Loss: 11848473640960.0\n",
      "Epoch [37/50], Loss: 12130600728283.428\n",
      "Epoch [38/50], Loss: 11998966766445.715\n",
      "Epoch [39/50], Loss: 11968641499136.0\n",
      "Epoch [40/50], Loss: 11957252166217.143\n",
      "Epoch [41/50], Loss: 12215309116562.285\n",
      "Epoch [42/50], Loss: 12030559462546.285\n",
      "Epoch [43/50], Loss: 12150436565577.143\n",
      "Epoch [44/50], Loss: 11988569985609.143\n",
      "Epoch [45/50], Loss: 11969598998820.572\n",
      "Epoch [46/50], Loss: 11936825156754.285\n",
      "Epoch [47/50], Loss: 11923887226880.0\n",
      "Epoch [48/50], Loss: 11818786956434.285\n",
      "Epoch [49/50], Loss: 11869723033600.0\n",
      "Epoch [50/50], Loss: 11815899627520.0\n",
      "Decision Tree Validation - Accuracy: 0.9425493716337523 Precision: 0.9095744680851063 Recall: 0.9193548387096774 F1 Score: 0.9144385026737968\n",
      "Random Forest Validation - Accuracy: 0.9569120287253142 Precision: 0.95 Recall: 0.9193548387096774 F1 Score: 0.9344262295081968\n",
      "LDA Validation - Accuracy: 0.6912028725314183 Precision: 0.5813953488372093 Recall: 0.26881720430107525 F1 Score: 0.36764705882352944\n",
      "Naive Bayes Validation - Accuracy: 0.5008976660682226 Precision: 0.39545454545454545 Recall: 0.9354838709677419 F1 Score: 0.5559105431309904\n",
      "Logistic Regression Validation - Accuracy: 0.7271095152603232 Precision: 0.7931034482758621 Recall: 0.24731182795698925 F1 Score: 0.3770491803278688\n",
      "KNN Validation - Accuracy: 0.9533213644524237 Precision: 0.9 Recall: 0.967741935483871 F1 Score: 0.9326424870466321\n",
      "SVM Validation - Accuracy: 0.7755834829443446 Precision: 0.8765432098765432 Recall: 0.3817204301075269 F1 Score: 0.5318352059925093\n",
      "Extra Trees Validation - Accuracy: 0.9587073608617595 Precision: 0.9453551912568307 Recall: 0.9301075268817204 F1 Score: 0.9376693766937669\n",
      "MLP Validation - Accuracy: 0.5870736086175943 Precision: 0.3674698795180723 Recall: 0.3279569892473118 F1 Score: 0.3465909090909091\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Định nghĩa Autoencoder\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        encoding_dim1 = 50\n",
    "        encoding_dim2 = 30\n",
    "        encoding_dim3 = int(np.sqrt(input_dim)) + 1\n",
    "\n",
    "        # Lớp mã hóa\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, encoding_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim1, encoding_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim2, encoding_dim3),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # Lớp giải mã\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim3, encoding_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim2, encoding_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim1, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "# Hàm để áp dụng deep autoencoder và train classifiers\n",
    "def train_with_autoencoder(X_train, y_train, X_val, y_val, X_test, y_test, epochs=50, batch_size=256, alpha1=0.1):\n",
    "    input_dim = X_train.shape[1]\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Khởi tạo mô hình và các thành phần cần thiết\n",
    "    autoencoder = Autoencoder(input_dim).to(device)\n",
    "    optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Chuyển đổi dữ liệu DataFrame thành NumPy array và tensor\n",
    "    X_train_np = X_train.to_numpy()\n",
    "    X_val_np = X_val.to_numpy()\n",
    "    X_test_np = X_test.to_numpy()\n",
    "    y_train_np = y_train.to_numpy()\n",
    "    y_val_np = y_val.to_numpy()\n",
    "    y_test_np = y_test.to_numpy()\n",
    "    \n",
    "    # Tạo TensorDataset cho training và validation\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.tensor(X_train_np, dtype=torch.float32),\n",
    "        torch.tensor(X_train_np, dtype=torch.float32)  # Sử dụng dữ liệu đầu vào làm mục tiêu cho autoencoder\n",
    "    )\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.tensor(X_val_np, dtype=torch.float32),\n",
    "        torch.tensor(X_val_np, dtype=torch.float32)  # Sử dụng dữ liệu đầu vào làm mục tiêu cho autoencoder\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        autoencoder.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            encoded, outputs = autoencoder(inputs)\n",
    "            loss = alpha1 * criterion(outputs, targets)  # Chỉ sử dụng MSE loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {train_loss/len(train_loader)}')\n",
    "\n",
    "    # Encode dữ liệu\n",
    "    autoencoder.eval()\n",
    "    with torch.no_grad():\n",
    "        X_train_enc, _ = autoencoder(torch.tensor(X_train_np, dtype=torch.float32).to(device))\n",
    "        X_val_enc, _ = autoencoder(torch.tensor(X_val_np, dtype=torch.float32).to(device))\n",
    "        X_test_enc, _ = autoencoder(torch.tensor(X_test_np, dtype=torch.float32).to(device))\n",
    "    \n",
    "    # Train và đánh giá classifiers\n",
    "    results = []\n",
    "    for name, train_function in classifiers.items():\n",
    "        model, val_acc, val_pre, val_rec, val_f1, test_acc, test_pre, test_rec, test_f1 = train_function(X_train_enc.cpu().numpy(), y_train, X_val_enc.cpu().numpy(), y_val, X_test_enc.cpu().numpy(), y_test, save_data)\n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Validation Accuracy': val_acc,\n",
    "            'Validation Precision': val_pre,\n",
    "            'Validation Recall': val_rec,\n",
    "            'Validation F1 Score': val_f1,\n",
    "            'Test Accuracy': test_acc,\n",
    "            'Test Precision': test_pre,\n",
    "            'Test Recall': test_rec,\n",
    "            'Test F1 Score': test_f1\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Gọi hàm và lưu kết quả\n",
    "autoencoder_results = train_with_autoencoder(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "autoencoder_results_df = pd.DataFrame(autoencoder_results)\n",
    "autoencoder_results_df.to_csv(save_data + 'autoencoder_model_2018.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 11.100731713431221\n",
      "Epoch [2/10], Loss: 10.866159439086914\n",
      "Epoch [3/10], Loss: 10.621945108686175\n",
      "Epoch [4/10], Loss: 10.33474336351667\n",
      "Epoch [5/10], Loss: 10.14861365727016\n",
      "Epoch [6/10], Loss: 9.964658873421806\n",
      "Epoch [7/10], Loss: 9.89177485874721\n",
      "Epoch [8/10], Loss: 9.700197628566197\n",
      "Epoch [9/10], Loss: 9.601284572056361\n",
      "Epoch [10/10], Loss: 9.572335243225098\n",
      "Decision Tree Validation - Accuracy: 0.9353680430879713 Precision: 0.936046511627907 Recall: 0.8655913978494624 F1 Score: 0.8994413407821229\n",
      "Random Forest Validation - Accuracy: 0.947935368043088 Precision: 0.9289617486338798 Recall: 0.9139784946236559 F1 Score: 0.9214092140921409\n",
      "LDA Validation - Accuracy: 0.8096947935368043 Precision: 0.8846153846153846 Recall: 0.4946236559139785 F1 Score: 0.6344827586206897\n",
      "Naive Bayes Validation - Accuracy: 0.3644524236983842 Precision: 0.32857142857142857 Recall: 0.8655913978494624 F1 Score: 0.47633136094674555\n",
      "Logistic Regression Validation - Accuracy: 0.8096947935368043 Precision: 0.8846153846153846 Recall: 0.4946236559139785 F1 Score: 0.6344827586206897\n",
      "KNN Validation - Accuracy: 0.9443447037701975 Precision: 0.8974358974358975 Recall: 0.9408602150537635 F1 Score: 0.9186351706036745\n",
      "SVM Validation - Accuracy: 0.8150807899461401 Precision: 0.9029126213592233 Recall: 0.5 F1 Score: 0.643598615916955\n",
      "Extra Trees Validation - Accuracy: 0.9533213644524237 Precision: 0.925531914893617 Recall: 0.9354838709677419 F1 Score: 0.93048128342246\n",
      "MLP Validation - Accuracy: 0.822262118491921 Precision: 0.8372093023255814 Recall: 0.5806451612903226 F1 Score: 0.6857142857142857\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Định nghĩa Autoencoder\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        encoding_dim1 = 50\n",
    "        encoding_dim2 = 30\n",
    "        encoding_dim3 = int(np.sqrt(input_dim)) + 1\n",
    "\n",
    "        # Lớp mã hóa\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, encoding_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim1, encoding_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim2, encoding_dim3),\n",
    "            nn.ReLU()  # Đổi từ sigmoid thành ReLU để giữ giá trị dương\n",
    "        )\n",
    "        # Lớp giải mã\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim3, encoding_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim2, encoding_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim1, input_dim),\n",
    "            nn.Sigmoid()  # Giữ sigmoid cho đầu ra\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded, encoded\n",
    "\n",
    "class BinaryMagnetLoss(nn.Module):\n",
    "    def __init__(self, alpha=7.18, epsilon=1e-6):\n",
    "        super(BinaryMagnetLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, embeddings, labels):\n",
    "        _min_float = 1e-6\n",
    "        embeddings = embeddings.float()\n",
    "        batch_size = embeddings.size(0)\n",
    "\n",
    "        unique_labels = labels.unique()\n",
    "        batch_clusters = {label.item(): (labels == label).nonzero(as_tuple=True)[0] for label in unique_labels}\n",
    "\n",
    "        num_instances = 0.0\n",
    "        stdev = torch.zeros(1).to(embeddings.device)\n",
    "        c_means = torch.stack([torch.mean(embeddings[batch_clusters[label.item()]], dim=0) for label in unique_labels]).to(embeddings.device)\n",
    "\n",
    "        for label in unique_labels:\n",
    "            cluster_indices = batch_clusters[label.item()]\n",
    "            for i in cluster_indices:\n",
    "                stdev += (embeddings[i] - c_means[unique_labels.tolist().index(label)]).norm(p=2).pow(2)\n",
    "                num_instances += 1.0\n",
    "\n",
    "        stdev = stdev / (num_instances - 1.0)\n",
    "        stdev = -2.0 * (stdev + self.epsilon)  # Add epsilon to avoid division by zero\n",
    "\n",
    "        loss = torch.zeros(1).to(embeddings.device)\n",
    "        denom = [torch.zeros(1).to(embeddings.device) for _ in range(batch_size)]\n",
    "\n",
    "        for label in unique_labels:\n",
    "            cluster_indices = batch_clusters[label.item()]\n",
    "            for i in cluster_indices:\n",
    "                for other_label in unique_labels:\n",
    "                    if other_label != label:\n",
    "                        denom[i] += ((embeddings[i] - c_means[unique_labels.tolist().index(other_label)]).norm().pow(2) / stdev).exp()\n",
    "\n",
    "                # Avoid log(0) by clamping values\n",
    "                loss_term = (((embeddings[i] - c_means[unique_labels.tolist().index(label)]).norm().pow(2) / stdev - self.alpha).exp() / (denom[i] + self.epsilon)).log().clamp(max=0.0)\n",
    "                if torch.isnan(loss_term) or torch.isinf(loss_term):\n",
    "                    continue  # Skip this term if it's invalid\n",
    "                loss -= loss_term\n",
    "\n",
    "        loss /= num_instances\n",
    "        return loss\n",
    "\n",
    "\n",
    "# Custom Loss Function combining Cross-Entropy and Binary Magnet Loss\n",
    "def custom_loss(y_true, y_pred, embeddings, alpha1=1.0, alpha2=1.0):\n",
    "    ce_loss = F.cross_entropy(y_pred, y_true.long())  # Tính toán Cross-Entropy Loss\n",
    "    magnet_loss_fn = BinaryMagnetLoss()\n",
    "    magnet_loss_value = magnet_loss_fn(embeddings, y_true)\n",
    "    return alpha1 * ce_loss + alpha2 * magnet_loss_value\n",
    "\n",
    "# Hàm để áp dụng deep autoencoder và train classifiers\n",
    "def train_with_autoencoder(X_train, y_train, X_val, y_val, X_test, y_test, classifiers, save_data, epochs=10, batch_size=256, alpha1=1.0, alpha2=1.0):\n",
    "    input_dim = X_train.shape[1]\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    autoencoder = Autoencoder(input_dim).to(device)\n",
    "    optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "\n",
    "    X_train_np = X_train.to_numpy()\n",
    "    X_val_np = X_val.to_numpy()\n",
    "    y_train_np = y_train.to_numpy()\n",
    "    y_val_np = y_val.to_numpy()\n",
    "\n",
    "    train_dataset = TensorDataset(\n",
    "        torch.tensor(X_train_np, dtype=torch.float32),\n",
    "        torch.tensor(y_train_np, dtype=torch.float32)\n",
    "    )\n",
    "    val_dataset = TensorDataset(\n",
    "        torch.tensor(X_val_np, dtype=torch.float32),\n",
    "        torch.tensor(y_val_np, dtype=torch.float32)\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        autoencoder.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs, encoded = autoencoder(inputs)\n",
    "            loss = custom_loss(labels, outputs, encoded, alpha1=alpha1, alpha2=alpha2)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {train_loss/len(train_loader)}')\n",
    "\n",
    "    autoencoder.eval()\n",
    "    with torch.no_grad():\n",
    "        X_train_enc = autoencoder.encoder(torch.tensor(X_train_np, dtype=torch.float32).to(device)).cpu().numpy()\n",
    "        X_val_enc = autoencoder.encoder(torch.tensor(X_val_np, dtype=torch.float32).to(device)).cpu().numpy()\n",
    "        X_test_enc = autoencoder.encoder(torch.tensor(X_test.to_numpy(), dtype=torch.float32).to(device)).cpu().numpy()\n",
    "\n",
    "    results = []\n",
    "    for name, train_function in classifiers.items():\n",
    "        model, val_acc, val_pre, val_rec, val_f1, test_acc, test_pre, test_rec, test_f1 = train_function(X_train_enc, y_train, X_val_enc, y_val, X_test_enc, y_test, save_data)\n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Validation Accuracy': val_acc,\n",
    "            'Validation Precision': val_pre,\n",
    "            'Validation Recall': val_rec,\n",
    "            'Validation F1 Score': val_f1,\n",
    "            'Test Accuracy': test_acc,\n",
    "            'Test Precision': test_pre,\n",
    "            'Test Recall': test_rec,\n",
    "            'Test F1 Score': test_f1\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "# Gọi hàm và lưu kết quả\n",
    "autoencoder_results = train_with_autoencoder(X_train, y_train, X_val, y_val, X_test, y_test, classifiers, save_data)\n",
    "autoencoder_results_df = pd.DataFrame(autoencoder_results)\n",
    "autoencoder_results_df.to_csv(save_data + 'autoencoder_Binary_Magnet_Loss_results_2018.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
