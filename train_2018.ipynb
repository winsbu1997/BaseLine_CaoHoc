{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headers match. Proceeding with merge.\n",
      "['Benign' 'Brute Force -Web' 'Brute Force -XSS' 'SQL Injection']\n",
      "Label\n",
      "Benign              2090330\n",
      "Brute Force -Web        611\n",
      "Brute Force -XSS        230\n",
      "SQL Injection            87\n",
      "Name: count, dtype: int64\n",
      "2090330\n",
      "928\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from utils.data_preprocessing import load_data, preprocess_data_2018\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "save_data = \"save_data_2018/\"\n",
    "\n",
    "\n",
    "cicids2018_path_v1 = 'data/Thursday-22-02-2018_TrafficForML_CICFlowMeter.csv'\n",
    "cicids2018_path_v2 = 'data/Friday-23-02-2018_TrafficForML_CICFlowMeter.csv'\n",
    "if(not os.path.isfile(save_data + \"web_attacks.csv\")):\n",
    "    # Optionally, remove duplicate rows\n",
    "    data_2018_v1 = load_data(cicids2018_path_v1)\n",
    "    data_2018_v2 = load_data(cicids2018_path_v2)\n",
    "\n",
    "    print(\"Headers match. Proceeding with merge.\")\n",
    "    merged_df = pd.concat([data_2018_v1, data_2018_v2], axis=0, ignore_index=True)\n",
    "    merged_df = merged_df.drop_duplicates()\n",
    "    merged_df.to_csv(save_data + \"merge.csv\", index=False)\n",
    "    \n",
    "    merge_data = load_data(save_data + \"merge.csv\")\n",
    "    preprocess_data_2018(merge_data, save_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(save_data + 'web_attacks.csv')\n",
    "df['Label'] = df['Label'].apply(lambda x: 0 if x == 'Benign' else 1)\n",
    "\n",
    "\n",
    "normal_df = df[df['Label'] == 0]\n",
    "attack_df = df[df['Label'] != 0]\n",
    "\n",
    "num_attack = len(attack_df)\n",
    "\n",
    "num_normal = 2 * num_attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if num_normal > len(normal_df):\n",
    "    raise ValueError(\"num_normal exceeds the number of rows in normal_df.\")\n",
    "normal_df = normal_df.sample(n=num_normal, random_state=42)\n",
    "\n",
    "# Ensure 'Label' column exists\n",
    "if 'Label' not in normal_df.columns or 'Label' not in attack_df.columns:\n",
    "    raise ValueError(\"Both DataFrames must have a 'Label' column.\")\n",
    "\n",
    "# Combine and shuffle the dataset\n",
    "balanced_df = pd.concat([normal_df, attack_df]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Split the balanced dataset\n",
    "if 'Label' not in balanced_df.columns:\n",
    "    raise ValueError(\"The 'Label' column is missing from the balanced DataFrame.\")\n",
    "train_df, temp_df = train_test_split(balanced_df, test_size=0.4, random_state=42, stratify=balanced_df['Label'])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['Label'])\n",
    "\n",
    "# Ensure enough data for balancing\n",
    "train_normal_df = train_df[train_df['Label'] == 0]\n",
    "train_attack_df = train_df[train_df['Label'] == 1]\n",
    "\n",
    "if len(train_attack_df) == 0:\n",
    "    raise ValueError(\"There are no attack instances in the training set.\")\n",
    "\n",
    "num_train_attack = len(train_attack_df)\n",
    "num_train_normal = 2 * num_train_attack\n",
    "\n",
    "# Ensure we don't try to sample more rows than are available\n",
    "if len(train_normal_df) < num_train_normal:\n",
    "    num_train_normal = len(train_normal_df)  # Adjust to available number of rows\n",
    "\n",
    "train_normal_df = train_normal_df.sample(n=num_train_normal, random_state=42)\n",
    "train_df = pd.concat([train_normal_df, train_attack_df]).sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set class distribution: {0: 1113, 1: 557}\n",
      "Validation set class distribution: {0: 371, 1: 186}\n",
      "Testing set class distribution: {0: 372, 1: 185}\n"
     ]
    }
   ],
   "source": [
    "# Print the count of each class in the training, validation, and testing sets\n",
    "for name, df in zip([\"Training\", \"Validation\", \"Testing\"], [train_df, val_df, test_df]):\n",
    "    unique, counts = np.unique(df['Label'], return_counts=True)\n",
    "    print(f\"{name} set class distribution:\", dict(zip(unique, counts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the train, validation, and test sets to CSV files\n",
    "train_df.to_csv(save_data +  'train_set.csv', index=False)\n",
    "val_df.to_csv(save_data +  'val_set.csv', index=False)\n",
    "test_df.to_csv(save_data + 'test_set.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns (excluding the label column)\n",
    "feature_cols = [col for col in df.columns if col != 'Label']\n",
    "\n",
    "# Extract features and labels for training, validation, and testing sets\n",
    "X_train = train_df[feature_cols]\n",
    "y_train = train_df['Label']\n",
    "X_val = val_df[feature_cols]\n",
    "y_val = val_df['Label']\n",
    "X_test = test_df[feature_cols]\n",
    "y_test = test_df['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classifiers\n",
    "from models.decision_tree import train_decision_tree\n",
    "from models.random_forest import train_random_forest\n",
    "from models.lda import train_lda\n",
    "from models.naive_bayes import train_naive_bayes\n",
    "from models.logistic_regression import train_logistic_regression\n",
    "from models.knn import train_knn\n",
    "from models.svm import train_svm\n",
    "from models.extra_trees import train_extra_trees\n",
    "from models.bagging import train_bagging\n",
    "from models.mlp import train_mlp\n",
    "\n",
    "classifiers = {\n",
    "    'Decision Tree': train_decision_tree,\n",
    "    'Random Forest': train_random_forest,\n",
    "    'Linear Discriminant Analysis': train_lda,\n",
    "    'Naive Bayes': train_naive_bayes,\n",
    "    'Logistic Regression': train_logistic_regression,\n",
    "    'K-Nearest Neighbors': train_knn,\n",
    "    'Support Vector Machine': train_svm,\n",
    "    'Extra Trees Classifier': train_extra_trees,\n",
    "    'Bagging Classifier': train_bagging,\n",
    "    'Multi-layer Perceptron': train_mlp\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Validation - Accuracy: 0.9228007181328546 Precision: 0.8864864864864865 Recall: 0.8817204301075269 F1 Score: 0.8840970350404312\n",
      "Random Forest Validation - Accuracy: 0.9371633752244165 Precision: 0.9171270718232044 Recall: 0.8924731182795699 F1 Score: 0.9046321525885559\n",
      "LDA Validation - Accuracy: 0.9228007181328546 Precision: 0.8325581395348837 Recall: 0.9623655913978495 F1 Score: 0.8927680798004988\n",
      "Naive Bayes Validation - Accuracy: 0.6732495511669659 Precision: 0.5056179775280899 Recall: 0.967741935483871 F1 Score: 0.6642066420664207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\a\\anaconda3\\envs\\AI\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Validation - Accuracy: 0.8132854578096947 Precision: 0.7808219178082192 Recall: 0.6129032258064516 F1 Score: 0.6867469879518072\n",
      "KNN Validation - Accuracy: 0.9353680430879713 Precision: 0.9213483146067416 Recall: 0.8817204301075269 F1 Score: 0.9010989010989011\n",
      "SVM Validation - Accuracy: 0.7917414721723519 Precision: 0.9861111111111112 Recall: 0.3817204301075269 F1 Score: 0.5503875968992248\n",
      "Extra Trees Validation - Accuracy: 0.9371633752244165 Precision: 0.9171270718232044 Recall: 0.8924731182795699 F1 Score: 0.9046321525885559\n",
      "MLP Validation - Accuracy: 0.7989228007181328 Precision: 0.7936507936507936 Recall: 0.5376344086021505 F1 Score: 0.6410256410256411\n"
     ]
    }
   ],
   "source": [
    "def model_10Classifier():\n",
    "    results = []\n",
    "    for name, train_function in classifiers.items():\n",
    "        model, val_acc, val_pre, val_rec, val_f1, test_acc, test_pre, test_rec, test_f1 = train_function(X_train, y_train, X_val, y_val, X_test, y_test, save_data)\n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Validation Accuracy': val_acc,\n",
    "            'Validation Precision': val_pre,\n",
    "            'Validation Recall': val_rec,\n",
    "            'Validation F1 Score': val_f1,\n",
    "            \"Test Accuracy\": test_acc,\n",
    "            \"Test Precision\": test_pre,\n",
    "            \"Test Recall\": test_rec,\n",
    "            \"Test F1 Score\": test_f1,\n",
    "        })\n",
    "\n",
    "    # Save results to CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(save_data + '10Classsifer_results_2018.csv', index=False)\n",
    "    \n",
    "model_10Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_pca shape: (1670, 20)\n",
      "X_val_pca shape: (557, 20)\n",
      "X_test_pca shape: (557, 20)\n",
      "Decision Tree Validation - Accuracy: 0.9245960502692998 Precision: 0.8956043956043956 Recall: 0.8763440860215054 F1 Score: 0.8858695652173914\n",
      "Random Forest Validation - Accuracy: 0.9317773788150808 Precision: 0.9204545454545454 Recall: 0.8709677419354839 F1 Score: 0.8950276243093923\n",
      "LDA Validation - Accuracy: 0.7809694793536804 Precision: 0.8902439024390244 Recall: 0.3924731182795699 F1 Score: 0.5447761194029851\n",
      "Naive Bayes Validation - Accuracy: 0.39676840215439857 Precision: 0.33766233766233766 Recall: 0.8387096774193549 F1 Score: 0.48148148148148145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\a\\anaconda3\\envs\\AI\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Validation - Accuracy: 0.8186714542190305 Precision: 0.8695652173913043 Recall: 0.5376344086021505 F1 Score: 0.6644518272425249\n",
      "KNN Validation - Accuracy: 0.9299820466786356 Precision: 0.9248554913294798 Recall: 0.8602150537634409 F1 Score: 0.8913649025069638\n",
      "SVM Validation - Accuracy: 0.7917414721723519 Precision: 0.9861111111111112 Recall: 0.3817204301075269 F1 Score: 0.5503875968992248\n",
      "Extra Trees Validation - Accuracy: 0.9228007181328546 Precision: 0.9085714285714286 Recall: 0.8548387096774194 F1 Score: 0.8808864265927978\n",
      "MLP Validation - Accuracy: 0.8599640933572711 Precision: 0.921875 Recall: 0.6344086021505376 F1 Score: 0.7515923566878981\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assume the classifiers dictionary and train functions are already defined\n",
    "# Define a function to apply PCA and train classifiers\n",
    "\n",
    "def train_with_pca(X_train, y_train, X_val, y_val, X_test, y_test, n_components=20):\n",
    "    X_combined = np.concatenate((X_train, X_val, X_test), axis=0)\n",
    "    y_combined = np.concatenate((y_train, y_val, y_test), axis=0)\n",
    "\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_combined_pca = pca.fit_transform(X_combined)\n",
    "\n",
    "    # Bước 3: Tách lại dữ liệu thành các tập train, validation và test\n",
    "    X_train_pca = X_combined_pca[:len(X_train)]\n",
    "    X_val_pca = X_combined_pca[len(X_train):len(X_train)+len(X_val)]\n",
    "    X_test_pca = X_combined_pca[len(X_train)+len(X_val):]\n",
    "\n",
    "    # Kiểm tra kích thước của các tập dữ liệu sau khi PCA\n",
    "    print(\"X_train_pca shape:\", X_train_pca.shape)\n",
    "    print(\"X_val_pca shape:\", X_val_pca.shape)\n",
    "    print(\"X_test_pca shape:\", X_test_pca.shape)\n",
    "    \n",
    "    results = []\n",
    "    for name, train_function in classifiers.items():\n",
    "        model, val_acc, val_pre, val_rec, val_f1, test_acc, test_pre, test_rec, test_f1 = train_function(X_train_pca, y_train, X_val_pca, y_val, X_test_pca, y_test, save_data)\n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Validation Accuracy': val_acc,\n",
    "            'Validation Precision': val_pre,\n",
    "            'Validation Recall': val_rec,\n",
    "            'Validation F1 Score': val_f1,\n",
    "            'Test Accuracy': test_acc,\n",
    "            'Test Precision': test_pre,\n",
    "            'Test Recall': test_rec,\n",
    "            'Test F1 Score': test_f1\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Call the function and save results\n",
    "pca_results = train_with_pca(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "pca_results_df = pd.DataFrame(pca_results)\n",
    "pca_results_df.to_csv(save_data + 'pca_model_results_2018.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n",
      "Epoch 1/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - loss: 124269300285440.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 2/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 126587357888512.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 3/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 134347122278400.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 4/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 133368599543808.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 5/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 125627634024448.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 6/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 128293365874688.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 7/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 115728724262912.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 8/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 136027553398784.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 9/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 136099410214912.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 10/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 121724465053696.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 11/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 133988727390208.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 12/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 132822937370624.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 13/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 129705453813760.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 14/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 122213713838080.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 15/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 127632083517440.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 16/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 125551440297984.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 17/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 134551812702208.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 18/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 128937317367808.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 19/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 126777158533120.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 20/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 128987472855040.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 21/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 127896005902336.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 22/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 124583654981632.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 23/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 134177513013248.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 24/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 129468341420032.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 25/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 125666758492160.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 26/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 130097948393472.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 27/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 129907275333632.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 28/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 130944996474880.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 29/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 129554249154560.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 30/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 131812361764864.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 31/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 132963496886272.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 32/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 135621880315904.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 33/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 122866548867072.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 34/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 129970600935424.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 35/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 132635586199552.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 36/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 127065902809088.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 37/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 122817014136832.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 38/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 131767818256384.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 39/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 129778518589440.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 40/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 132082147786752.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 41/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 129544962965504.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 42/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 129201382359040.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 43/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 133419426119680.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 44/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 126825451749376.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 45/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 123329188986880.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 46/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 125671581941760.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 47/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 128614624395264.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 48/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 127168864583680.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 49/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 136685052493824.0000 - val_loss: 130405491539968.0000\n",
      "Epoch 50/50\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 126612834091008.0000 - val_loss: 130405491539968.0000\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 951us/step\n",
      "Decision Tree Validation - Accuracy: 0.9138240574506283 Precision: 0.8920454545454546 Recall: 0.8440860215053764 F1 Score: 0.8674033149171271\n",
      "Random Forest Validation - Accuracy: 0.9245960502692998 Precision: 0.9044943820224719 Recall: 0.8655913978494624 F1 Score: 0.8846153846153846\n",
      "LDA Validation - Accuracy: 0.770197486535009 Precision: 0.8452380952380952 Recall: 0.3817204301075269 F1 Score: 0.5259259259259259\n",
      "Naive Bayes Validation - Accuracy: 0.5709156193895871 Precision: 0.43614457831325304 Recall: 0.9731182795698925 F1 Score: 0.6023294509151415\n",
      "Logistic Regression Validation - Accuracy: 0.7666068222621185 Precision: 0.7 Recall: 0.5268817204301075 F1 Score: 0.6012269938650306\n",
      "KNN Validation - Accuracy: 0.9317773788150808 Precision: 0.9204545454545454 Recall: 0.8709677419354839 F1 Score: 0.8950276243093923\n",
      "SVM Validation - Accuracy: 0.7827648114901257 Precision: 0.922077922077922 Recall: 0.3817204301075269 F1 Score: 0.5399239543726235\n",
      "Extra Trees Validation - Accuracy: 0.9228007181328546 Precision: 0.9085714285714286 Recall: 0.8548387096774194 F1 Score: 0.8808864265927978\n",
      "MLP Validation - Accuracy: 0.7648114901256733 Precision: 0.8160919540229885 Recall: 0.3817204301075269 F1 Score: 0.5201465201465202\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define a function to build and train a deep autoencoder\n",
    "def build_autoencoder(input_dim):\n",
    "    # 3 lớp ẩn\n",
    "    encoding_dim1 = 50\n",
    "    encoding_dim2 = 30\n",
    "    # lớp trung gian\n",
    "    encoding_dim3 = int(np.sqrt(input_dim)) + 1\n",
    "    \n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    \n",
    "    # Lớp mã hóa\n",
    "    encoded = Dense(encoding_dim1, activation='relu')(input_layer)\n",
    "    encoded = Dense(encoding_dim2, activation='relu')(encoded)\n",
    "    encoded = Dense(encoding_dim3, activation='relu')(encoded)\n",
    "    \n",
    "    # Lớp giải mã\n",
    "    decoded = Dense(encoding_dim2, activation='relu')(encoded)\n",
    "    decoded = Dense(encoding_dim1, activation='relu')(decoded)\n",
    "    decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "    \n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    encoder = Model(input_layer, encoded)\n",
    "    \n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    return autoencoder, encoder\n",
    "\n",
    "# Define a function to apply deep autoencoder and train classifiers\n",
    "def train_with_autoencoder(X_train, y_train, X_val, y_val, X_test, y_test, epochs=50, batch_size=256):\n",
    "    input_dim = X_train.shape[1]\n",
    "    print(input_dim)\n",
    "    autoencoder, encoder = build_autoencoder(input_dim)\n",
    "    \n",
    "    autoencoder.fit(X_train, X_train, epochs=epochs, batch_size=batch_size, shuffle=True, validation_data=(X_val, X_val), verbose=1)\n",
    "    \n",
    "    X_train_enc = encoder.predict(X_train)\n",
    "    X_val_enc = encoder.predict(X_val)\n",
    "    X_test_enc = encoder.predict(X_test)\n",
    "    \n",
    "    results = []\n",
    "    for name, train_function in classifiers.items():\n",
    "        model, val_acc, val_pre, val_rec, val_f1, test_acc, test_pre, test_rec, test_f1 = train_function(X_train_enc, y_train, X_val_enc, y_val, X_test_enc, y_test, save_data)\n",
    "        results.append({\n",
    "            'Model': name,\n",
    "            'Validation Accuracy': val_acc,\n",
    "            'Validation Precision': val_pre,\n",
    "            'Validation Recall': val_rec,\n",
    "            'Validation F1 Score': val_f1,\n",
    "            'Test Accuracy': test_acc,\n",
    "            'Test Precision': test_pre,\n",
    "            'Test Recall': test_rec,\n",
    "            'Test F1 Score': test_f1\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Call the function and save results\n",
    "autoencoder_results = train_with_autoencoder(X_train, y_train, X_val, y_val, X_test, y_test)\n",
    "autoencoder_results_df = pd.DataFrame(autoencoder_results)\n",
    "autoencoder_results_df.to_csv(save_data + 'autoencoder_model_results_2018.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
